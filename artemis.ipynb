{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"msmarco-passage\"\n",
    "\n",
    "A passage ranking benchmark with a collection of 8.8 million passages and question queries. Most relevance judgments are shallow (typically at most 1-2 per query), but the TREC Deep Learning track adds deep judgments. Evaluation typically conducted using MRR@10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset carregado com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Erro ao iterar sobre as queries: 'charmap' codec can't decode byte 0x9d in position 6067: character maps to <undefined>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Queries muito similares detectadas:\n",
      "  [1] who determines disability for social security\n",
      "  [2] how much is social security for disability\n",
      "------------------------------------------------------------\n",
      "üîÑ Queries muito similares detectadas:\n",
      "  [1] who was scheherazade\n",
      "  [2] who is scheherazade\n",
      "------------------------------------------------------------\n",
      "üîÑ Queries muito similares detectadas:\n",
      "  [1] average cost of assisted living in washington state\n",
      "  [2] average cost of assisted living in kansas\n",
      "------------------------------------------------------------\n",
      "üîÑ Queries muito similares detectadas:\n",
      "  [1] how much is social security for disability\n",
      "  [2] social security's definition of disability\n",
      "------------------------------------------------------------\n",
      "üîÑ Queries muito similares detectadas:\n",
      "  [1] gastric bypass cost\n",
      "  [2] gastric bypass surgery cost\n",
      "------------------------------------------------------------\n",
      "üîÑ Substituindo 5 queries muito similares...\n",
      "‚úÖ Substituindo: social security's definition of disability ‚Üí how did Tecumseh's brother the prophet die\n",
      "‚úÖ Substituindo: gastric bypass surgery cost ‚Üí figley definition of compassion fatigue\n",
      "‚úÖ Substituindo: how much is social security for disability ‚Üí how many electron rings does copper have\n",
      "‚úÖ Substituindo: average cost of assisted living in kansas ‚Üí where can radiation be found?\n",
      "‚úÖ Substituindo: who is scheherazade ‚Üí who was maria gaetana agnesi\n",
      "Total de queries v√°lidas: 46932\n",
      "Total de queries selecionadas: 1000\n",
      "Query ID: 1043190 | Texto: who led the anabaptist movement in switzerland | Tipo: PERSON\n",
      "Query ID: 950852 | Texto: when is kelly clarkson album coming out | Tipo: PERSON\n",
      "Query ID: 1027622 | Texto: who explained the behavior of atoms for the first time | Tipo: PERSON\n",
      "Query ID: 1028600 | Texto: who invented air conditioning | Tipo: PERSON\n",
      "Query ID: 1044355 | Texto: who makes the crusader fifth wheel | Tipo: PERSON\n",
      "Query ID: 232713 | Texto: how far is wilmington nc to lillington nc | Tipo: PERSON\n",
      "Query ID: 1050016 | Texto: who sang mad world originally | Tipo: PERSON\n",
      "Query ID: 1032812 | Texto: who is fbi director comey | Tipo: PERSON\n",
      "Query ID: 940510 | Texto: when did richard nixon become president | Tipo: PERSON\n",
      "Query ID: 1037469 | Texto: who is regina calcaterra s father | Tipo: PERSON\n"
     ]
    }
   ],
   "source": [
    "import ir_datasets\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Carrega o modelo de NLP para reconhecimento de entidades\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "NUM_QUERIES_PER_TYPE = 200  # Ajuste conforme necess√°rio\n",
    "SIMILARITY_THRESHOLD = 0.7  # Limite para considerar queries como muito parecidas\n",
    "\n",
    "query_types = {\"PERSON\": [], \"LOCATION\": [], \"DESCRIPTION\": [], \"NUMERIC\": [], \"ENTITY\": []}\n",
    "queries_annotated = []  # Lista de queries com anota√ß√µes\n",
    "selected_queries = []   # Queries selecionadas\n",
    "\n",
    "# Define o nome do dataset\n",
    "dataset_name = \"msmarco-passage-v2/train\"\n",
    "\n",
    "try:\n",
    "    dataset = ir_datasets.load(dataset_name)\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o dataset {dataset_name}: {e}\")\n",
    "    dataset = None\n",
    "\n",
    "def decode_utf8_safe(text):\n",
    "    \"\"\"Tenta converter texto para UTF-8 de forma segura, ignorando caracteres n√£o v√°lidos.\"\"\"\n",
    "    try:\n",
    "        return text.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return text.encode(\"utf-8\", \"ignore\").decode(\"utf-8\", \"ignore\")\n",
    "\n",
    "def classify_query(text):\n",
    "    \"\"\"Classifica a query automaticamente em uma das categorias: NUMERIC, ENTITY, LOCATION, PERSON, DESCRIPTION.\"\"\"\n",
    "    \n",
    "    # Regras baseadas em palavras-chave\n",
    "    if re.search(r'\\b(who|whom)\\b', text, re.IGNORECASE):\n",
    "        return \"PERSON\"\n",
    "    if re.search(r'\\b(where|location|city|country)\\b', text, re.IGNORECASE):\n",
    "        return \"LOCATION\"\n",
    "    if re.search(r'\\b(what|define|describe|meaning)\\b', text, re.IGNORECASE):\n",
    "        return \"DESCRIPTION\"\n",
    "    if re.search(r'\\b(how many|how much|\\d+|percent|percentage)\\b', text, re.IGNORECASE):\n",
    "        return \"NUMERIC\"\n",
    "\n",
    "    # Usa NLP para an√°lise de entidades\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"PERSON\"]:\n",
    "            return \"PERSON\"\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]:  # GPE = Pa√≠ses, cidades, estados | LOC = Locais\n",
    "            return \"LOCATION\"\n",
    "        elif ent.label_ in [\"CARDINAL\", \"QUANTITY\", \"PERCENT\", \"MONEY\"]:\n",
    "            return \"NUMERIC\"\n",
    "        elif ent.label_ in [\"ORG\", \"PRODUCT\", \"EVENT\"]:\n",
    "            return \"ENTITY\"\n",
    "\n",
    "    # Se n√£o se encaixar em nenhuma categoria espec√≠fica, assume DESCRIPTION\n",
    "    return \"DESCRIPTION\"\n",
    "\n",
    "# Processando queries\n",
    "if dataset:\n",
    "    try:\n",
    "        for query in dataset.queries_iter():\n",
    "            text_utf8 = decode_utf8_safe(query.text)\n",
    "\n",
    "            if text_utf8 == query.text:  # Apenas queries v√°lidas\n",
    "                q_type = classify_query(text_utf8)  # Classifica√ß√£o autom√°tica\n",
    "                queries_annotated.append((query.query_id, text_utf8, q_type))\n",
    "                query_types[q_type].append((query.query_id, text_utf8))\n",
    "            else:\n",
    "                logging.warning(f\"Query ID {query.query_id} cont√©m caracteres inv√°lidos e foi ignorada.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro ao iterar sobre as queries: {e}\")\n",
    "\n",
    "# Selecionando queries proporcionalmente\n",
    "for q_type, queries in query_types.items():\n",
    "    # Filtra apenas as queries anotadas do tipo correspondente\n",
    "    filtered_queries = [q for q in queries_annotated if q[2] == q_type]\n",
    "\n",
    "    # Seleciona aleatoriamente queries desse tipo, garantindo que cada uma contenha (id, texto, tipo)\n",
    "    selected_queries.extend(random.sample(filtered_queries, min(NUM_QUERIES_PER_TYPE, len(filtered_queries))))\n",
    "\n",
    "\n",
    "# Criando matriz TF-IDF para medir similaridade\n",
    "query_texts = [query_text for _, query_text,_ in selected_queries]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(query_texts)\n",
    "\n",
    "# Calculando similaridade entre todas as queries selecionadas\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Identificando queries muito similares\n",
    "to_replace = set()\n",
    "for i in range(len(selected_queries)):\n",
    "    for j in range(i + 1, len(selected_queries)):\n",
    "        if similarity_matrix[i, j] > SIMILARITY_THRESHOLD:\n",
    "            print(f\"üîÑ Queries muito similares detectadas:\")\n",
    "            print(f\"  [1] {selected_queries[i][1]}\")\n",
    "            print(f\"  [2] {selected_queries[j][1]}\")\n",
    "            print(\"-\" * 60)\n",
    "            to_replace.add(j)  # Marca a segunda query como redundante\n",
    "\n",
    "# Substituindo queries muito parecidas\n",
    "if to_replace:\n",
    "    print(f\"üîÑ Substituindo {len(to_replace)} queries muito similares...\")\n",
    "    for index in sorted(to_replace, reverse=True):  # Substituir de tr√°s para frente para evitar problemas de √≠ndice\n",
    "        _, text, q_type = selected_queries[index]\n",
    "        \n",
    "        # Busca uma alternativa no mesmo tipo\n",
    "        available_queries = [q for q in query_types[q_type] if q not in selected_queries]\n",
    "        if available_queries:\n",
    "            replacement = random.choice(available_queries)\n",
    "            print(f\"‚úÖ Substituindo: {text} ‚Üí {replacement[1]}\")\n",
    "            selected_queries[index] = replacement\n",
    "        else:\n",
    "            # Se n√£o houver substituto, apenas remove\n",
    "            print(f\"‚ö†Ô∏è Nenhuma substitui√ß√£o dispon√≠vel para: {text}. Removendo.\")\n",
    "            selected_queries.pop(index)\n",
    "\n",
    "\n",
    "# Exibindo resultado\n",
    "print(f\"Total de queries v√°lidas: {len(queries_annotated)}\")\n",
    "print(f\"Total de queries selecionadas: {len(selected_queries)}\")\n",
    "\n",
    "# Exemplo de sa√≠da das queries selecionadas\n",
    "for query_id, query_text,query_type in selected_queries[:10]:  # Exibir 10 exemplos\n",
    "    print(f\"Query ID: {query_id} | Texto: {query_text} | Tipo: {query_type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Coletar os textos dos documentos\u001b[39;00m\n\u001b[32m     12\u001b[39m doc_texts = {}  \u001b[38;5;66;03m# {doc_id: doc_text}\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdocs_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mquery_to_docs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Apenas docs necess√°rios\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_texts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdoc_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_utf8_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ir_datasets\\util\\__init__.py:147\u001b[39m, in \u001b[36mDocstoreSplitter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ir_datasets\\datasets\\msmarco_passage_v2.py:69\u001b[39m, in \u001b[36mMsMarcoV2Passages.docs_iter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     67\u001b[39m file = tarf.extractfile(record)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m gzip.open(file) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparse_msmarco_passage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\gzip.py:427\u001b[39m, in \u001b[36mGzipFile.readline\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadline\u001b[39m(\u001b[38;5;28mself\u001b[39m, size=-\u001b[32m1\u001b[39m):\n\u001b[32m    426\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_not_closed()\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_buffer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\_compression.py:68\u001b[39m, in \u001b[36mDecompressReader.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view.cast(\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] = data\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\gzip.py:537\u001b[39m, in \u001b[36m_GzipReader.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    535\u001b[39m     uncompress = \u001b[38;5;28mself\u001b[39m._decompressor.decompress(buf, size)\n\u001b[32m    536\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m     uncompress = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._decompressor.unused_data != \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    540\u001b[39m     \u001b[38;5;66;03m# Prepend the already read bytes to the fileobj so they can\u001b[39;00m\n\u001b[32m    541\u001b[39m     \u001b[38;5;66;03m# be seen by _read_eof() and _read_gzip_header()\u001b[39;00m\n\u001b[32m    542\u001b[39m     \u001b[38;5;28mself\u001b[39m._fp.prepend(\u001b[38;5;28mself\u001b[39m._decompressor.unused_data)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "NUM_DOCS_PER_QUERY = 3\n",
    "query_to_docs = {}  # {query_id: [doc_id, doc_text]}\n",
    "\n",
    "# Mapear queries para documentos relevantes\n",
    "for qrel in dataset.qrels_iter():\n",
    "    if qrel.query_id in [q[0] for q in selected_queries]:  # Apenas queries selecionadas\n",
    "        if qrel.query_id not in query_to_docs:\n",
    "            query_to_docs[qrel.query_id] = []\n",
    "        query_to_docs[qrel.query_id].append(qrel.doc_id)\n",
    "\n",
    "# Coletar os textos dos documentos\n",
    "doc_texts = {}  # {doc_id: doc_text}\n",
    "for doc in dataset.docs_iter():\n",
    "    if doc.doc_id in {doc_id for docs in query_to_docs.values() for doc_id in docs}:  # Apenas docs necess√°rios\n",
    "        doc_texts[doc.doc_id] = decode_utf8_safe(doc.text)\n",
    "\n",
    "# Criar JSONL para treino do modelo\n",
    "with open(\"train_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for query_id, query_text, query_type in selected_queries:\n",
    "        doc_list = query_to_docs.get(query_id, [])[:NUM_DOCS_PER_QUERY]  # Seleciona at√© 5 docs\n",
    "        for doc_id in doc_list:\n",
    "            f.write(json.dumps({\n",
    "                \"query_id\": query_id,\n",
    "                \"query_text\": query_text,\n",
    "                \"query_type\": query_type,\n",
    "                \"doc_id\": doc_id,\n",
    "                \"doc_text\": doc_texts.get(doc_id, \"\")\n",
    "            }) + \"\\n\")\n",
    "\n",
    "print(f\"Dataset salvo com {len(selected_queries)} queries e {sum(len(docs) for docs in query_to_docs.values())} documentos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objetivo: criar uma rotina que gere um conjunto de dados v√°lido de menor escala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passo a passo:\n",
    "\n",
    "- Pegar s√≥ o que √© escrito no nosso alfabeto (utf-8)\n",
    "\n",
    "- Ordenar por tipo de pergunta\n",
    "\n",
    "- Selecionar quantidades  iguais de cada tipo\n",
    "\n",
    "- Trocar perguntas similares usando TF-IDF\n",
    "\n",
    "- Escolher quais documentos ser√£o utilizados usando algum crit√©rio (A ser feito)\n",
    "\n",
    "- Salvar em um formato mais leve para processar os dados (A ser feito)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
